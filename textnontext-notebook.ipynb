{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# dog means text and cat means nontext\n\n\nThis Kernel for someone want to deep dive into image classification. I use CNN for classification model. If you found this Kernel helpful please up vote it. If you have some feedback and question don't forget to comment below. \n\nI have simplier model with \n* https://www.kaggle.com/uysimty/get-start-image-classification","metadata":{}},{"cell_type":"markdown","source":"# Import Library","metadata":{"_uuid":"fe76d1d1ded592430e7548feacfa38dc42f085d9"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport random\nimport os\nprint(os.listdir(\"../input\"))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-28T12:57:06.063182Z","iopub.execute_input":"2022-01-28T12:57:06.063457Z","iopub.status.idle":"2022-01-28T12:57:07.745048Z","shell.execute_reply.started":"2022-01-28T12:57:06.063403Z","shell.execute_reply":"2022-01-28T12:57:07.74422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Constants","metadata":{}},{"cell_type":"code","source":"FAST_RUN = False\nIMAGE_WIDTH=128\nIMAGE_HEIGHT=128\nIMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\nIMAGE_CHANNELS=3\n","metadata":{"execution":{"iopub.status.busy":"2022-01-28T12:57:07.749322Z","iopub.execute_input":"2022-01-28T12:57:07.751601Z","iopub.status.idle":"2022-01-28T12:57:07.758061Z","shell.execute_reply.started":"2022-01-28T12:57:07.751541Z","shell.execute_reply":"2022-01-28T12:57:07.757124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Traning Data","metadata":{"_uuid":"7335a579cc0268fba5d34d6f7558f33c187eedb3"}},{"cell_type":"code","source":"filenames = os.listdir(\"../input/text-and-non-text-images\")\ncategories = []\nigd=['dog366.jpg','dog468.jpg','dog188.jpg','dog478.jpg','cat15.jpg','cat66.jpg','dog177.jpg','cat24.jpg']\nfor filename in filenames:\n    category = filename[:3]\n    if filename in igd:\n        print(\"move\")\n        continue\n    if category == 'dog':\n        categories.append(1)\n    else:\n        categories.append(0)\nfor i in igd:\n    filenames.remove(i)\ndf = pd.DataFrame({\n    'filename': filenames,\n    'category': categories\n})","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-01-28T12:57:07.762195Z","iopub.execute_input":"2022-01-28T12:57:07.764942Z","iopub.status.idle":"2022-01-28T12:57:07.8923Z","shell.execute_reply.started":"2022-01-28T12:57:07.764884Z","shell.execute_reply":"2022-01-28T12:57:07.891466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"_uuid":"915bb9ba7063ab4d5c07c542419ae119003a5f98","execution":{"iopub.status.busy":"2022-01-28T12:57:12.567011Z","iopub.execute_input":"2022-01-28T12:57:12.567304Z","iopub.status.idle":"2022-01-28T12:57:12.582939Z","shell.execute_reply.started":"2022-01-28T12:57:12.567255Z","shell.execute_reply":"2022-01-28T12:57:12.582073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"_uuid":"72bf69e817f67f5a2eaff8561217e22077248553","execution":{"iopub.status.busy":"2022-01-28T12:57:13.419599Z","iopub.execute_input":"2022-01-28T12:57:13.419888Z","iopub.status.idle":"2022-01-28T12:57:13.433384Z","shell.execute_reply.started":"2022-01-28T12:57:13.419833Z","shell.execute_reply":"2022-01-28T12:57:13.432467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### See Total In count","metadata":{"_uuid":"a999484fc35b73373fafe2253ae9db7ff46fdb90"}},{"cell_type":"code","source":"df['category'].value_counts().plot.bar()","metadata":{"_uuid":"fa26f0bc7a6d835a24989790b20f3c6f32946f45","execution":{"iopub.status.busy":"2022-01-28T12:57:14.383843Z","iopub.execute_input":"2022-01-28T12:57:14.384185Z","iopub.status.idle":"2022-01-28T12:57:14.66013Z","shell.execute_reply.started":"2022-01-28T12:57:14.384127Z","shell.execute_reply":"2022-01-28T12:57:14.658946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From our data we have 12000 cats and 12000 dogs","metadata":{"_uuid":"3a08da58107777a1dd05c4a4bf5c484484923cac"}},{"cell_type":"markdown","source":"# See sample image","metadata":{"_uuid":"400a293df3c8499059d9175f3915187074efd971"}},{"cell_type":"code","source":"sample = random.choice(filenames)\nprint(sample)\nimage = load_img(\"../input/text-and-non-text-images/\"+sample)\nplt.imshow(image)","metadata":{"_uuid":"602b40f7353871cb161c60b5237f0da0096b2f47","execution":{"iopub.status.busy":"2022-01-28T12:57:15.855007Z","iopub.execute_input":"2022-01-28T12:57:15.855304Z","iopub.status.idle":"2022-01-28T12:57:16.08521Z","shell.execute_reply.started":"2022-01-28T12:57:15.855248Z","shell.execute_reply":"2022-01-28T12:57:16.084478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Model\n\n<img src=\"https://i.imgur.com/ebkMGGu.jpg\" width=\"100%\"/>","metadata":{"_uuid":"b244e6b7715a04fc6df92dd6dfa3d35c473ca600"}},{"cell_type":"markdown","source":"* **Input Layer**: It represent input image data. It will reshape image into single diminsion array. Example your image is 64x64 = 4096, it will convert to (4096,1) array.\n* **Conv Layer**: This layer will extract features from image.\n* **Pooling Layer**: This layerreduce the spatial volume of input image after convolution.\n* **Fully Connected Layer**: It connect the network from a layer to another layer\n* **Output Layer**: It is the predicted values layer. ","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n\n# model = Sequential()\n\n\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n\n# model.add(Conv2D(64, (3, 3), activation='relu'))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n\n# model.add(Conv2D(128, (3, 3), activation='relu'))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n\n# model.add(Flatten())\n# model.add(Dense(512, activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.5))\n# model.add(Dense(2, activation='softmax')) # 2 because we have cat and dog classes\n\n# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\n# model.summary()\n\nmodel=Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64,(3,3),activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128,(3,3),activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n  optimizer='rmsprop',metrics=['accuracy'])","metadata":{"_uuid":"8c9f833c1441b657c779844912d0b8028218d454","execution":{"iopub.status.busy":"2022-01-28T13:02:19.532267Z","iopub.execute_input":"2022-01-28T13:02:19.532543Z","iopub.status.idle":"2022-01-28T13:02:20.222259Z","shell.execute_reply.started":"2022-01-28T13:02:19.532491Z","shell.execute_reply":"2022-01-28T13:02:20.221438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Callbacks","metadata":{"_uuid":"bd496f6c65888a969be3703135b0b03a8a1190c8"}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau","metadata":{"_uuid":"9aa032f0f6da539d23918890d2d131cc3aac8c7a","execution":{"iopub.status.busy":"2022-01-28T13:03:47.012919Z","iopub.execute_input":"2022-01-28T13:03:47.013288Z","iopub.status.idle":"2022-01-28T13:03:47.017313Z","shell.execute_reply.started":"2022-01-28T13:03:47.013231Z","shell.execute_reply":"2022-01-28T13:03:47.016592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Early Stop**\n\nTo prevent over fitting we will stop the learning after 10 epochs and val_loss value not decreased","metadata":{"_uuid":"76c9ba4fb7f930c96b2c3e0d6b68ed9fa6a4227b"}},{"cell_type":"code","source":"earlystop = EarlyStopping(patience=10)","metadata":{"_uuid":"3421c5ec428da6c0d8cc1184179a9caff1e01d1c","execution":{"iopub.status.busy":"2022-01-28T13:03:48.80973Z","iopub.execute_input":"2022-01-28T13:03:48.810035Z","iopub.status.idle":"2022-01-28T13:03:48.813835Z","shell.execute_reply.started":"2022-01-28T13:03:48.809956Z","shell.execute_reply":"2022-01-28T13:03:48.813101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Learning Rate Reduction**\n\nWe will reduce the learning rate when then accuracy not increase for 2 steps","metadata":{"_uuid":"51d3fe52e911286433cedf6e47332948a253361e"}},{"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","metadata":{"_uuid":"8010a5661ad8924d2db24af0f3c00b1593b38901","execution":{"iopub.status.busy":"2022-01-28T13:03:49.236175Z","iopub.execute_input":"2022-01-28T13:03:49.236383Z","iopub.status.idle":"2022-01-28T13:03:49.24021Z","shell.execute_reply.started":"2022-01-28T13:03:49.236341Z","shell.execute_reply":"2022-01-28T13:03:49.239498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [earlystop, learning_rate_reduction]","metadata":{"_uuid":"a79cc604199469789f183096d863f7248e5f6aab","execution":{"iopub.status.busy":"2022-01-28T13:03:50.039847Z","iopub.execute_input":"2022-01-28T13:03:50.040175Z","iopub.status.idle":"2022-01-28T13:03:50.044545Z","shell.execute_reply.started":"2022-01-28T13:03:50.040116Z","shell.execute_reply":"2022-01-28T13:03:50.043715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data","metadata":{}},{"cell_type":"markdown","source":"Because we will use image genaretor `with class_mode=\"categorical\"`. We need to convert column category into string. Then imagenerator will convert it one-hot encoding which is good for our classification. \n\nSo we will convert 1 to dog and 0 to cat","metadata":{}},{"cell_type":"code","source":"df[\"category\"] = df[\"category\"].replace({0: 'cat', 1: 'dog'}) ","metadata":{"execution":{"iopub.status.busy":"2022-01-28T13:03:51.322876Z","iopub.execute_input":"2022-01-28T13:03:51.323262Z","iopub.status.idle":"2022-01-28T13:03:51.33035Z","shell.execute_reply.started":"2022-01-28T13:03:51.323203Z","shell.execute_reply":"2022-01-28T13:03:51.328358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)\ntrain_df = train_df.reset_index(drop=True)\nvalidate_df = validate_df.reset_index(drop=True)","metadata":{"_uuid":"4eeb7af8dcf02c4ef5ca744c8305c51a2f5cedef","execution":{"iopub.status.busy":"2022-01-28T13:03:52.826278Z","iopub.execute_input":"2022-01-28T13:03:52.826556Z","iopub.status.idle":"2022-01-28T13:03:52.834646Z","shell.execute_reply.started":"2022-01-28T13:03:52.826505Z","shell.execute_reply":"2022-01-28T13:03:52.833751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['category'].value_counts().plot.bar()","metadata":{"_uuid":"b84836337441705eda9d2e655665ffa14d9feead","execution":{"iopub.status.busy":"2022-01-28T13:03:53.672743Z","iopub.execute_input":"2022-01-28T13:03:53.673046Z","iopub.status.idle":"2022-01-28T13:03:53.979813Z","shell.execute_reply.started":"2022-01-28T13:03:53.672984Z","shell.execute_reply":"2022-01-28T13:03:53.978998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validate_df['category'].value_counts().plot.bar()","metadata":{"_uuid":"19cf03f9a3c39532d6e2d06bd30be49a5afd9d57","execution":{"iopub.status.busy":"2022-01-28T13:03:54.577507Z","iopub.execute_input":"2022-01-28T13:03:54.577759Z","iopub.status.idle":"2022-01-28T13:03:54.770448Z","shell.execute_reply.started":"2022-01-28T13:03:54.577712Z","shell.execute_reply":"2022-01-28T13:03:54.769616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_train = train_df.shape[0]\ntotal_validate = validate_df.shape[0]\nbatch_size=15","metadata":{"_uuid":"ae3dec0361f0443132d0309d3b883ee80070cf9f","execution":{"iopub.status.busy":"2022-01-28T13:03:55.059952Z","iopub.execute_input":"2022-01-28T13:03:55.060217Z","iopub.status.idle":"2022-01-28T13:03:55.064222Z","shell.execute_reply.started":"2022-01-28T13:03:55.06017Z","shell.execute_reply":"2022-01-28T13:03:55.063333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Traning Generator","metadata":{"_uuid":"ff760be9104f7d9492467b8d9d3405011aa77d11"}},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n    rotation_range=15,\n    rescale=1./255,\n    shear_range=0.1,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    width_shift_range=0.1,\n    height_shift_range=0.1\n)\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    train_df, \n    \"../input/text-and-non-text-images\", \n    x_col='filename',\n    y_col='category',\n    target_size=IMAGE_SIZE,\n    class_mode='categorical',\n    batch_size=batch_size\n)","metadata":{"_uuid":"4d1c7818703a8a4bac5c036fdea45972aa9e5e9e","execution":{"iopub.status.busy":"2022-01-28T13:03:55.824419Z","iopub.execute_input":"2022-01-28T13:03:55.824894Z","iopub.status.idle":"2022-01-28T13:03:56.197021Z","shell.execute_reply.started":"2022-01-28T13:03:55.824825Z","shell.execute_reply":"2022-01-28T13:03:56.194871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation Generator","metadata":{"_uuid":"859c7b2857939c19fd2e3bb32839c9f7deb5aa3f"}},{"cell_type":"code","source":"validation_datagen = ImageDataGenerator(rescale=1./255)\nvalidation_generator = validation_datagen.flow_from_dataframe(\n    validate_df, \n    \"../input/text-and-non-text-images\", \n    x_col='filename',\n    y_col='category',\n    target_size=IMAGE_SIZE,\n    class_mode='categorical',\n    batch_size=batch_size\n)","metadata":{"_uuid":"7925e16bcacc89f4484fb6fe47e54d6420af732e","execution":{"iopub.status.busy":"2022-01-28T13:03:56.479534Z","iopub.execute_input":"2022-01-28T13:03:56.482231Z","iopub.status.idle":"2022-01-28T13:03:56.567846Z","shell.execute_reply.started":"2022-01-28T13:03:56.482164Z","shell.execute_reply":"2022-01-28T13:03:56.567093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# See how our generator work","metadata":{"_uuid":"6e17fc1f002fedd60febb78fee5e81770640b909"}},{"cell_type":"code","source":"example_df = train_df.sample(n=1).reset_index(drop=True)\nexample_generator = train_datagen.flow_from_dataframe(\n    example_df, \n    \"../input/text-and-non-text-images\", \n    x_col='filename',\n    y_col='category',\n    target_size=IMAGE_SIZE,\n    class_mode='categorical'\n)","metadata":{"_uuid":"4252cce168ab65f88e44a8ebc2672607bc852af4","execution":{"iopub.status.busy":"2022-01-28T13:03:57.193552Z","iopub.execute_input":"2022-01-28T13:03:57.193842Z","iopub.status.idle":"2022-01-28T13:03:57.207553Z","shell.execute_reply.started":"2022-01-28T13:03:57.193786Z","shell.execute_reply":"2022-01-28T13:03:57.206728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nfor i in range(0, 15):\n    plt.subplot(5, 3, i+1)\n    for X_batch, Y_batch in example_generator:\n        image = X_batch[0]\n        print(image[0][0])\n        print(image.shape)\n        print(type(image))\n        plt.imshow(image)\n        break\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"23d923dba747f8b47dc75569244cecc6f70df321","execution":{"iopub.status.busy":"2022-01-28T13:23:15.191366Z","iopub.execute_input":"2022-01-28T13:23:15.191653Z","iopub.status.idle":"2022-01-28T13:23:17.906539Z","shell.execute_reply.started":"2022-01-28T13:23:15.1916Z","shell.execute_reply":"2022-01-28T13:23:17.905907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seem to be nice ","metadata":{"_uuid":"810ddf1373d9db470ed48da4f30ca5a6c1274435"}},{"cell_type":"markdown","source":"# Fit Model","metadata":{"_uuid":"5cd8df64e794ed17de326b613a9819e7da977a0e"}},{"cell_type":"code","source":"epochs=3 if FAST_RUN else 50\nhistory = model.fit_generator(\n    train_generator, \n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=total_validate//batch_size,\n    steps_per_epoch=total_train//batch_size,\n    callbacks=callbacks\n)","metadata":{"_uuid":"0836a4cc8aa0abf603e0f96573c0c4ff383ad56b","execution":{"iopub.status.busy":"2022-01-28T13:03:59.91216Z","iopub.execute_input":"2022-01-28T13:03:59.912598Z","iopub.status.idle":"2022-01-28T13:07:17.521067Z","shell.execute_reply.started":"2022-01-28T13:03:59.912552Z","shell.execute_reply":"2022-01-28T13:07:17.520308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Model","metadata":{"_uuid":"aa1fbc4081ae0de2993188b2bf658a0be5bc0687"}},{"cell_type":"code","source":"model.save(\"model.model\")","metadata":{"_uuid":"67575a4decdaf79a915d23151626b784ffa82642","execution":{"iopub.status.busy":"2022-01-28T13:36:40.338746Z","iopub.execute_input":"2022-01-28T13:36:40.339079Z","iopub.status.idle":"2022-01-28T13:36:40.998733Z","shell.execute_reply.started":"2022-01-28T13:36:40.339017Z","shell.execute_reply":"2022-01-28T13:36:40.99791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Virtualize Training","metadata":{"_uuid":"1b76c0a9040bc0babf0a453e567e41e22f8a1e0e"}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\nax1.plot(history.history['loss'], color='b', label=\"Training loss\")\nax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\nax1.set_xticks(np.arange(1, epochs, 1))\nax1.set_yticks(np.arange(0, 1, 0.1))\n\nax2.plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax2.plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nax2.set_xticks(np.arange(1, epochs, 1))\n\nlegend = plt.legend(loc='best', shadow=True)\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"79055f2dc3e2abb47bea758e0464c86ca42ab431","execution":{"iopub.status.busy":"2022-01-21T22:33:04.442667Z","iopub.status.idle":"2022-01-21T22:33:04.443298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Testing Data","metadata":{"_uuid":"764dc66e4b2bc558f3a0f90b80bb802f5b3d45a8"}},{"cell_type":"code","source":"test_filenames = os.listdir(\"../input/test1/test1\")\ntest_df = pd.DataFrame({\n    'filename': test_filenames\n})\nnb_samples = test_df.shape[0]","metadata":{"_uuid":"c35e70d3e1e4834dbbf840fa0ea08c049bfcd915","execution":{"iopub.status.busy":"2022-01-21T22:33:04.444039Z","iopub.status.idle":"2022-01-21T22:33:04.444949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Testing Generator","metadata":{"_uuid":"291bc3996dce8d05e174b27d64f03996d3e8038e"}},{"cell_type":"code","source":"test_gen = ImageDataGenerator(rescale=1./255)\ntest_generator = test_gen.flow_from_dataframe(\n    test_df, \n    \"../input/test1/test1/\", \n    x_col='filename',\n    y_col=None,\n    class_mode=None,\n    target_size=IMAGE_SIZE,\n    batch_size=batch_size,\n    shuffle=False\n)","metadata":{"_uuid":"52249ec1c35fb1be3adef386be245de3794e55aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom numpy import asarray\nimport cv2\n# numpy array\nimg = cv2.imread('../input/text-and-non-text-images/dog166.jpg')\n# asarray() class is used to convert\n# PIL images into NumPy arrays\n# numpydata = asarray(img)/255 #\nres = cv2.resize(img, dsize=(128, 128))\nnumpydata = np.expand_dims(res, axis=0)/255\na=model.predict([numpydata])","metadata":{"execution":{"iopub.status.busy":"2022-01-21T23:13:40.813583Z","iopub.execute_input":"2022-01-21T23:13:40.813874Z","iopub.status.idle":"2022-01-21T23:13:40.852454Z","shell.execute_reply.started":"2022-01-21T23:13:40.813825Z","shell.execute_reply":"2022-01-21T23:13:40.851347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nplt.imshow(mpimg.imread('../input/text-and-non-text-images/dog134.jpg'))","metadata":{"execution":{"iopub.status.busy":"2022-01-21T23:14:17.754942Z","iopub.execute_input":"2022-01-21T23:14:17.755243Z","iopub.status.idle":"2022-01-21T23:14:18.062861Z","shell.execute_reply.started":"2022-01-21T23:14:17.75517Z","shell.execute_reply":"2022-01-21T23:14:18.062055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict","metadata":{"_uuid":"2fa580afca2931ec5ce374e732d8c1789d03f2ed"}},{"cell_type":"code","source":"","metadata":{"_uuid":"4782eb23fa7d003f0e2415d995894017edb2d896","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For categoral classication the prediction will come with probability of each category. So we will pick the category that have the highest probability with numpy average max","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will convert the predict category back into our generator classes by using `train_generator.class_indices`. It is the classes that image generator map while converting data into computer vision","metadata":{}},{"cell_type":"code","source":"label_map = dict((v,k) for k,v in train_generator.class_indices.items())\ntest_df['category'] = test_df['category'].replace(label_map)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From our prepare data part. We map data with `{1: 'dog', 0: 'cat'}`. Now we will map the result back to dog is 1 and cat is 0","metadata":{}},{"cell_type":"code","source":"test_df['category'] = test_df['category'].replace({ 'dog': 1, 'cat': 0 })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Virtaulize Result","metadata":{"_uuid":"b00add65fe765529e637c3a9904d710bb7eff1d8"}},{"cell_type":"code","source":"test_df['category'].value_counts().plot.bar()","metadata":{"_uuid":"d0bf6dd5ff344092fa0121f70bdd60fa5a40e29c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### See predicted result with images","metadata":{"_uuid":"ce72a83f80d6e012b12b82c8ee3365d671a3b307"}},{"cell_type":"code","source":"sample_test = test_df.head(18)\nsample_test.head()\nplt.figure(figsize=(12, 24))\nfor index, row in sample_test.iterrows():\n    filename = row['filename']\n    category = row['category']\n    img = load_img(\"../input/test1/test1/\"+filename, target_size=IMAGE_SIZE)\n    plt.subplot(6, 3, index+1)\n    plt.imshow(img)\n    plt.xlabel(filename + '(' + \"{}\".format(category) + ')' )\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"98b41dc83075e6297137fb45bf703c313dd4ae28","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"_uuid":"d1ca25943e73aa20a37f9fb8670ee430caeaaf1f"}},{"cell_type":"code","source":"submission_df = test_df.copy()\nsubmission_df['id'] = submission_df['filename'].str.split('.').str[0]\nsubmission_df['label'] = submission_df['category']\nsubmission_df.drop(['filename', 'category'], axis=1, inplace=True)\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"_uuid":"cce9f3e2ffff0693d79d84590ed71fbbca7c3c7c","trusted":true},"execution_count":null,"outputs":[]}]}